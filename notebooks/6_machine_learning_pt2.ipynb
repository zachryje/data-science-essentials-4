{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf81d2b",
   "metadata": {},
   "source": [
    "# Regularization and Tree-Based Models\n",
    "\n",
    "In this notebook, we'll try out some additional models on the heart disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cm import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c39c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('../data/Heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b26ff9",
   "metadata": {},
   "source": [
    "Recall the variable descriptions:\n",
    "\n",
    "|Variable | Description |\n",
    "|---|---|\n",
    "| Age | age in years |\n",
    "| Sex | Sex (0 = female; 1 = male;) |\n",
    "| ChestPain | chest pain type |\n",
    "| RestBP | resting blood pressure (in mm Hg on admission to the hospital) |\n",
    "| Chol | serum cholestoral in mg/dl |\n",
    "| Fbs | fasting blood sugar > 120 mg/dl (1 = true; 0 = false) |\n",
    "| RestECG | resting electrocardiographic results (0: normal; 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV); 2: showing probable or definite left ventricular hypertrophy by Estes' criteria) |\n",
    "| MaxHR | maximum heart rate achieved |\n",
    "| ExAng | exercise induced angina (0 = no, 1 = yes;) |\n",
    "| Oldpeak | ST depression induced by exercise relative to rest |\n",
    "| Slope | the slope of the peak exercise ST segment (1: upsloping; 2: flat; 3: downsloping) |\n",
    "| Ca | number of major vessels (0-3) colored by flourosopy |\n",
    "| Thal | Thallium stress test  | \n",
    "| AHD | Presence of heart disease (target) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = ['ChestPain', 'RestECG', 'Slope', 'Thal']\n",
    "heart = pd.get_dummies(heart, columns = categorical_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0675c85",
   "metadata": {},
   "source": [
    "### Logistic Regression from Last Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart.drop(columns = ['AHD'])\n",
    "y = heart['AHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc49756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify = y,     # Keep the same proportions of the target in the training and test data\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter = 10000)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df613741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd03dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddbe9b",
   "metadata": {},
   "source": [
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4a0a6",
   "metadata": {},
   "source": [
    "A decision tree model is fit in much the same way as a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6237342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state = 321).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a93d34",
   "metadata": {},
   "source": [
    "First, let's see how it does on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f69bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "plot_confusion_matrix(y_train, tree.predict(X_train), labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a983f09",
   "metadata": {},
   "source": [
    "Since decision trees are so flexible, they will often perfectly predict the training data. Remember though, that what we care about is how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred, labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, tree.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256dee14",
   "metadata": {},
   "source": [
    "We do see a drop in model performance on the test data. This is a sign of overfitting.\n",
    "\n",
    "To correct for this problem, we can take an ensemble approach, which means that we will build many decision trees on subsets of the features and data and then average the predictions of all of the trees. This will force our model to try and find more general patterns that will work on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 321)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred, labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, forest.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec3b85",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We see some improvement over using a single tree, but we could do better. Random forests have a lot of hyperparameters that can be tuned to improve out model. Here are a few of these parameters:\n",
    "\n",
    "* **n_estimators:** Number of decision trees to train. Default is 10. More trees = less variance, but slower to train and predict\n",
    "* **max_depth:** Maximum depth (number of splits). By default, there is no max depth.\n",
    "* **min_samples_leaf:** Minimum number of samples per leaf. Setting this higher keeps the decision trees from paying too much attention to any single data point.\n",
    "\n",
    "These parameters can be tuned to try to improve the model that you get, and there are ways to automatically tune these parameters. See, for example, sklearn's GridSearchCV or RandomSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 1000, max_depth = 5, min_samples_leaf = 5, random_state = 321)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred, labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c791d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, forest.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3584b",
   "metadata": {},
   "source": [
    "A nice perk of using random forest models is that we can see which features are the most important in making predictions.\n",
    "\n",
    "**Caution:** Feature importances tell which features the model is using, but not _how_ it is using those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'variable': X.columns,\n",
    "                           'importance': forest.feature_importances_}).sort_values('importance', ascending = False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,8))\n",
    "sns.barplot(data = importances,\n",
    "            x = 'importance', y = 'variable', ax = ax, edgecolor = 'black')\n",
    "plt.title('Random Forest Feature Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267dab9",
   "metadata": {},
   "source": [
    "## Appendix: Regularized Regression\n",
    "\n",
    "Now, let's try out a penalized regression model. \n",
    "\n",
    "We'll do a LASSO regression model, which penalizes in a way that tends to force some of the coefficients to be zero.\n",
    "\n",
    "Since LASSO penalizes the coefficients, which depend on the magnitude of the varaibles, we should first scale our predictors so that they are on comparable scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e534f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=10000,\n",
    "                            penalty = 'l1',             # LASSO penalty\n",
    "                            C = 0.05,                   # Lower C = stronger penalty\n",
    "                           solver = 'saga')\n",
    "logreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, labels = ['No', 'Yes'], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad898f6d",
   "metadata": {},
   "source": [
    "With this model, we get a slighty lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b45f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, logreg.predict_proba(X_test_scaled)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7755f17",
   "metadata": {},
   "source": [
    "We also get a slightly lower ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'variable': X.columns,\n",
    "    'coefficient': logreg.coef_[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1cfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefficients[coefficients['coefficient'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118982",
   "metadata": {},
   "source": [
    "However, we end up with a much simpler model overall compared to the model above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
